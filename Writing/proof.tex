\subsection{Counter-Examples and Model Selections}

%To prove our hypothesis, we use counterexamples. Specifically, we want to disable and/or
 
%TODO (SON) need to check oe more time, make it stronger
%We conducted experiments to provide counterexamples to empirically
%validate that hypothesis. We choose the language migration task from
%Java to C\texttt{\#} because they are the most popular and similar
%languages and if BLEU does not work for this migration, it will not
%likely work for other pairs of programming languages.

%Son: a good metric SHOULD be independent from models and translation tasks
%So, we just confidently pick the most popular language migration task (without explanations)
We conducted experiments to provide counterexamples to empirically
validate that hypothesis. If BLEU is a good metric for language
migration, it should be independent from SMT-based migration models
and programming languages. Therefore, choosing programming languages
does not affect our assessment for BLEU. In this work, we choose the
language migration task from Java to C\texttt{\#} which is considered
as one of the most popular tasks in language migration.

%
%We will show that BLEU score is not strongly correlated to human
%judgments on the semantic accuracy of the migrated code. The judgments
%from human subjects are based on the estimation of the efforts that
%need to be put on to correct the resulting migrated code as well as
%how close the result is with respect to the given correct solution in
%the ground truth.
%
%First,
To address {\bf RQ1}, we provide counterexamples to show that BLEU score is
not strongly correlated to the semantic accuracy of the migrated
code. That is, for a particular translation model, the higher BLEU
score does not lead to the higher semantic accuracy of translated
results, and the higher semantic accuracy results are also not
necessary to lead to higher BLEU scores.
%
%We also show that under some circumstances an improvement in BLEU is
%not sufficient to reflect an improvement in code migration quality,
%and in other circumstances that it is not necessary to improve BLEU in
%order to achieve an improvement in migration~quality.
%
%Furthermore, under some circumstances an improvement in BLEU is
%not sufficient to reflect an improvement in code migration quality,
%and in other circumstances that it is not necessary to improve BLEU in
%order to achieve an improvement in migration~quality.
%

{\bf Higher BLEU score does not necessarily reflects higher semantic
  accuracy.}  We chose the migration models that focus on phrase
translation, for example, lpSMT~\cite{fse13-nier,karaivanov14} that
adapts phrase-to-phrase translation models~\cite{phrasal10}. This type
of models produces migrated code with high lexical accuracy, i.e.,
high correctness on the sequences of code tokens. However, several
tokens or sequences of tokens are placed in the incorrect locations.
This results in a higher in BLEU but with a lower semantic accuracy
in migrated code.

%TODO make code style consistent
\begin{figure}[t]
\centering
\begin{lstlisting}[basicstyle=\small\sffamily, stepnumber=1, numbers=left, language=Java, aboveskip=1pt,  belowskip=1pt, numbersep=-5pt]
  // Java code: ClientQueryResult.java
  public ClientQueryResult(Transaction ta, int initialSize){
    super(ta, initialSize);
  }

  // C# code: ClientQueryResult.cs
  public ClientQueryResult(Transaction ta, int initialSize) : base(ta, initialSize) {}

  // Translated by lpSMT:
  public ClientQueryResult(Transaction ta, int initialSize) : base(ta {, initialSize) ; }
\end{lstlisting}
\caption{lpSMT Example~\cite{fse13-nier}}
\label{fig:issueexample2}
\end{figure}

%{\bf Breaking a parent class' constructor call.}

Let us take an example of the resulting migrated code from the tool
lpSMT~\cite{fse13-nier} ({\em Call to Parent Class' Constructor
  \code{super(...)}}). Fig.~\ref{fig:issueexample2} shows an example
as the lpSMT model translates a call to the constructor of a parent
class via \code{super} (lines 2--4). In Java, a call to \code{super} is
made inside the method's body. In contrast, in C\texttt{\#}, a call to the
constructor is made via \code{base} and occurs in the method
signature, \ie prior to the method's body as in
\code{base(ta,initialSize) {}} (line 7). However, in the translation
version, this call was broken into two pieces: one in the method
signature \code{'base(ta'} and one in the method~body\\ \code{`, initialSize);'} (line 10). Thus, the translation code is
syntactically incorrect. In this case, lpSMT translated based on the
lexemes of tokens in the method signature and the body, however, does
not consider the entire syntactic unit of a constructor call to the
parent class in \code{super} for translation. For this example, the
BLEU score with $n$=4 is {\bf 0.673}. However, the translated code is not even
compiled due to that syntactic error. The effort to find the
misplacement and to correct this is not trivial.

{\bf Higher semantic accuracy is not necessarily reflected via higher BLEU
  score.}
%We chose the migration models/tools that focus on
%structures. Specifically, we picked mppSMT tool~\cite{ase15} that has
%high semantic accuracy but with a wide range of BLEU~scores.
%
To validate this argument, we chose the migration models/tools,
specifically, mppSMT tool~\cite{ase15}, that has high semantic
accuracy but probably with a wide range of BLEU~scores.
The key idea behind this choice is that mppSMT uses syntax-directed
translation, \ie it translates the code within syntactic structures
first, and then aggregates the translated code for all structures to
form the final migrated code. To improve semantic accuracy, mppSMT
integrates during the translation process the mappings of types and
API usages between two languages. This strategy has been efficient in
achieving higher syntactic and semantic accuracy than other existing
techniques~\cite{ase15}. An important characteristic of mppSMT is that
there is a considerable percentage of translated code that are
semantically correct, however, are different from the manual-migrated
code. Specifically, those correct code involves 1) code with different
local variables' names from a reference method, but all variables are
consistently renamed; 2) code with namespaces being added/deleted
to/from a type (e.g., \code{new P.A()} vs \code{new A()}); 3) code
with `\code{this}' being added/deleted to/from a field or method; and
4) code with different uses of syntactic units and APIs having the
same meaning, \eg field accesses versus getters or array accesses
versus method calls. These variations are all correct, however, the
BLEU scores among the pairs of the reference and translated code are
quite different.
%From those observations, we chose mppSMT for this experiment.

\input{datatable}

%--- Tien
%As seen in Table~\ref{tab:result}, {\tool} achieves good translation
%accuracy. {\bf 84.8--97.9\%} and {\bf 70-83\%} of the total numbers of
%translated methods are {\em syntactically} and {\em semantically
%  correct}, respectively. Among all total translated methods, there
%are {\em {\bf 26.3--51.2\%} that are exactly matched to the
%  C\texttt{\#} code written by the developers of the subject projects
%  in the oracle} (Table~\ref{tab:textmatch}).

%We examined the migrated results that are syntactically and
%semantically correct but differ from the manual-migrated code in the
%oracle. We found that they involve 1) code with different local
%variables' names from a reference method, but all variables are
%consistently renamed; 2) code with namespaces being added/ deleted
%to/from a type (e.g., \code{new P.A()} vs \code{new A()}); and 3) code
%with `\code{this}' being added/deleted to/from a field or method.
%Regarding EDR, only 3.7--14\% of the total number of tokens in the
%resulting code are incorrect (not shown).
%

To answer {\bf RQ2}, we provide a counterexample to empirically show the 
ineffectiveness of BLEU in comparing the migration quality of models. 
%
%Specifically, that is, two models can have the results with identical BLEU score but one will have worse translation quality.
Specifically, that is, {\em the equivalence of BLEU scores of the results translated 
by two models is not consistent with the equivalence in the translation quality 
of these models}.
%

To provide the counterexample, we construct a {\em permuted model,
  p-mppSMT}, from mppSMT, that applies the same principle as in
Callison-Burch {\em et al.}~\cite{Callison}. Their idea is that BLEU
{\em does not impose explicit constraints on the orders of all
  matching phrases} between the translated code and the reference
code. It enforces the order among the tokens in a $n$-gram, but does
not enforce the order among matching $n$-grams.
%
For~example, given the reference, "\textbf{A B C D}", the BLEU scores
of the two translated results, $t_1$: "\textbf{A B} E \textbf{C D}",
$t_2$: "\textbf{C D} E \textbf{A B}" are the same, because the number
of matching phrases such as "A B" and "C D" in the two results are
similar. Therefore, for a given reference code and a translated
result, there are many possible variants that have similar BLEU
scores with the translated one. However, for code migrated results,
the difference in the order of phrases between two results might lead
to the considerable semantic difference between them. For
instance, if "\textbf{A B}", "\textbf{E}", and "\textbf{C D}"
represent for the blocks code for opening a file, reading the opened
file, and closing the opened file, $t_1$ and $t_2$ are significantly
different in term of functionality. Hence, {\em by following that
  principle, the results from p-mppSMT achieve BLEU score equivalent
  with the results of mppSMT, but possibly with lower semantic~score}.
%We'll explain the results later.

%According to \cite{Callison}, BLEU accepts a large amount of variations because it only considers the numbers of matching n-grams but not their order. Thus, they argued that by reordering a hypothesis translation will not reduce the number of matching n-grams and thus will not reduce the overall Bleu
%score. 
%Following that principle, we modified the model mppSMT to generate a new model mppSMT2 that have identical BLEU score as the original one's. By having identical BLEU score as the original model, mppSMT2 will keep the correct n-grams, but swap positions of the incorrect ones. Although these phrases are incorrect in term of lexical, they may carry syntactical or semantic information. If they are moved into new position, their meaningful information will be lost. The problem is more severe for programming language which has strict syntactical rules and well semantics. For example, the tool mppSMT's result contains a partial name of a type instead of the fully qualified name in the manual migrated code. That partial name would be considered as incorrect phrase and relocated into a new position by the model mppSMT2. However, the type with partial name is only meaningful when placed at the correct location. Moving it will break the syntax/semantics of the code fragment.
%We will empirically compare the results of mppSMT and mppSMT2 to validate our argument: despite having identical BLEU scores, one model can still have worse translation quality than the other.
%
%Beside using a theoretical model, we also conduct the same experiment on two practical models: two SMT-based models belonging to the two popular groups of SMT: mppSMT~\cite{ase15}, which is based on phrase-based SMT, and GNMT~\cite{gnmt}, which is based on neural network models. 

%under certain
%circumstances an improvement in BLEU is not effective in reflecting an
%improvement in code migration quality. For this experiment, we pick
%the two SMT-based models belonging to the two popular groups of SMT:
%mppSMT~\cite{ase15}, which is based on phrase-based SMT, and
%GNMT~\cite{gnmt}, which is based on neural network models. ({\bf More
%  reasons?})

%With these insights, we chose those models and conducted experiments
%to empirically verify how the correlation between the BLEU scores and
%the human judgments on the semantic accuracy of the migrated code.

%Let us explain our data collection, settings, and metrics for our
%experiments.
