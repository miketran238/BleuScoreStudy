\section{Introduction}
\label{sec:intro}

Statistical Machine Translation (SMT)~\cite{smtbook} is a natural
language processing (NLP) approach~that uses statistical learning to
derive the translation ``rules'' from a training data and applies the
trained model to translate a sequence from the source language ($L_S$)
to the target one ($L_T$). SMT produces translated texts based on the
statistical models whose parameters are trained from a corpus of the
corresponding texts in two languages. SMT has been successful in
translating natural-language texts.  Google
Translate~\cite{googletranslate} is a SMT-based tool~that can accept
inputs in 15 natural languages and allows the translation of texts
into one of 53 languages. Microsoft Translator~\cite{mstranslator}
also supports instant translation for more than 40~languages.

The statistical machine translation community relies on the BLEU ({\em
  BiLingual Evaluation Understudy}) metric for the purpose of
evaluating SMT models and tools. {\em BLEU metric}, also called {\em
  BLEU~score}, measures translation quality by the accuracy of
translating text phrases to another with various phrases'
lengths. BLEU was shown to be highly correlated with human judgments
on the translated texts from natural-language SMT
tools~\cite{Papineni2002}. However, there exists criticism on BLEU
as Callison-Burch {\em et al.}~\cite{Callison} argued that an
improvement in BLEU metric is not sufficient nor necessary to show an
improvement in translation quality. Despite such criticism, BLEU
score still remains one of the most popular automated and inexpensive
metrics to evaluate the quality of translation models.


%However, Callison at el argued that we should not over-rely on Bleu
%score as an improvement in Bleu score is not sufficient nor necessary
%to show an in improvement in translation quality \cite {Callison06}.

In recent years, several researchers in Software Engineering (SE) and
Programming Languages have been exploring the NLP techniques and
models to build automated SE tools. SMT has been directly used or
adapted to be used to translate/migrate source code in different
programming
languages~\cite{fse13-nier,icse14-demo,karaivanov14,ase15,icsme16}. The
SE problem is called {\em language migration}. In the modern world of
computing, language migration is important. Software vendors often
develop a software product for multiple operating platforms in
different languages. For example, the same mobile app could be
developed for iOS (in Objective-C), Android (in Java), and Windows
Phone (in C\texttt{\#}).
%Rather than developing each version of a software product
%independently, it would be more economical to develop the product in
%one platform/language and then migrate to another.
Thus, nowadays, there is an increasing need for migration/translation
of source code from one programming language to another.
%

Unlike natural-language texts, source code follows syntactic rules and
has well-defined semantics with respect to programming languages. A
natural question is {\em how effective BLEU score is in~evaluating the
  results of migrated source code in language migration}. The answer
to this question is important because if it does, we could~establish
an automated metric to evaluate the quality of SMT-based code
migration tools, and otherwise, a relevant question should be raised:
{\em what is an alternative metric?} Unfortulately, there has not yet
any empirical evidence to either validate or invalidate the
effectiveness of BLEU score in applying to source code in language
migration.



%Nat Lang vs Prog Lang: Difference in structure.
%Question: Is bleu suitable for source code?  The answer will help
%build and evaluate MT-based Code Migration (Code to Code) If yes,
%^continue. If no, we need another metric.  To answer it, until now only
%informal statements. There hasn¡¦t been any empirical evidences to show
%its (in)validity.




Machine Translation (MT) is the use of computer program to translate
text or speech from one language to another. Bleu score evaluates the
quality of MT by calculating the modified n-grams precision and also
taking into account the length difference penalty. Traditionally, MT
is only applied to natural language, but now it is also used for
technical and programming language. One notable use of MT for SE tasks
is Code Migration. Even with that adaptation, SE community still
relies on Blue to evaluate the quality of MT. It is well known that
there is a significant difference between natural language and
programing language: programing language has structure, and
well-defined syntax. This leads to a question as whether Blue score is
suitable for SE task (Code Migration) or not. If it is, we could
continue to use it. Otherwise, we need another metric that is more
suitable for programing language. Hence, the answer to the question
above will help researchers and developers build and evaluate MT-based
Code Migration system better. Some has attempted to answer the
question by stating informal arguments toward the use of Bleu for SE
task \cite{}. However, up to date, there has not been any empirical
evidences to formally address the problem.

Bleu measures the lexical difference between machine generated code and referenced one. On the other hand, to measure the semantic similarity between them is the ultimate goal when evaluating quality of Code Migration system. 
 

Bleu was proved to be correlated with human judgments in natural language MT systems \cite {Papineni02}. However, Callison at el argued that we should not over-rely on Bleu score as an improvement in Bleu score is not sufficient nor necessary to show an in improvement in translation quality \cite {Callison06}. To validate the use of Bleu on SE tasks, we set up an experiment to manually judge the result of multiple MT systems and compare its to the Bleu score. Our result showed that Bleu score has weak correlation to human judgments across 

%\vspace{0.03in} {\em 1.}  \textbf{\code{BLEU}}. This is a popular
%metrics in SMT that measures translation quality by the accuracy of
%translating $n$-grams to $n$-grams with various values of $n$ (phrases
%to phrases):

% \[\code{BLEU} = BP.{e^{\frac{1}{n}(\log {P_1} + ... + \log {P_n})}}~\cite{bleu}\]
%where $BP$ is the {\em brevity penalty value}, which equals to 1 if
%the total length (i.e. the number of words) of the resulting sentences
%is longer than that of the {\em reference sentences} (i.e. the correct
%sentences in the oracle). Otherwise, it equals to the ratio between
%those two lengths. $P_i$ is the metrics for the overlapping between
%the bag of $i$-grams (repeating items are allowed) appearing in the
%resulting sentences and that of $i$-grams appearing in the reference
%sentences. Specifically, if $S^{i}_{ref}$ and $S^{i}_{trans}$ are the
%bags of $i$-grams appearing in the reference code and in the
%translation code respectively, $P_i$ = |$S^{i}_{ref}$ $\cap$
%$S^{i}_{trans}$|/|$S^{i}_{trans}$|. The value of \code{BLEU} is
%between 0-1. The higher it is, the higher the translation quality.

%Since $P_i$ represents the accuracy in translating phrases
%with $i$ consecutive words, the higher the value of $i$ is used, the
%better \code{BLEU} measures translation quality. For example, assume
%that a translation \code{Tr} has a high $P_1$ value but a
%low~$P_2$. That is, \code{Tr} has high word-to-word accuracy but low
%accuracy in translating 2-grams to 2-grams (e.g. the word order might
%not be respected in the result). Thus, using both $P_1$ and $P_2$ will
%measure \code{Tr} better than using only $P_1$. If
%translation~sen\-tences are shorter, \code{BP} is smaller and
%\code{BLEU} is smaller. If they are too long and more incorrect words
%occur, $P_i$ values are smaller, thus, \code{BLEU} is smaller. $P_i$s
%are computed for $i$=1-4.
