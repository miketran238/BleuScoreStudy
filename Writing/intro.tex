\section{Introduction}
\label{sec:intro}

Statistical Machine Translation (SMT)~\cite{smtbook} is a
natural~language processing (NLP) approach that uses statistical
learning to derive the translation ``rules'' from a training data and
applies the trained model to translate a sequence from the source
language to the target one.
%SMT produces translated texts based on the statistical models whose parameters
SMT models are trained from a corpus of corresponding texts in two
languages. SMT has been successful in translating natural-language
texts.  Google Translate~\cite{googletranslate} is an SMT-based
tool that support text translation of over 100
languages. Microsoft Translator~\cite{mstranslator} also supports
instant translation for more than 60 languages.

The statistical machine translation community relies on the {\em
  BiLingual Evaluation Understudy} (BLEU) metric for the purpose of
evaluating SMT models. {\em BLEU metric}, also called {\em
  BLEU~score}, measures translation quality by the accuracy of
translating text phrases to the reference with various phrases'
lengths. BLEU score was shown to be highly correlated with human judgments
on the translated texts from natural-language SMT
tools~\cite{Papineni2002}.
%
%However, there exists criticism on BLEU as Callison-Burch {\em et
% al.}~\cite{Callison} argued that an improvement in BLEU metric is
%not sufficient nor necessary to sho an improvement in translation
%quality. Despite such criticism,
BLEU remains a popular automated and inexpensive
metric to evaluate the quality of translation models.


%However, Callison at el argued that we should not over-rely on Bleu
%score as an improvement in Bleu score is not sufficient nor necessary
%to show an in improvement in translation quality \cite {Callison06}.

In recent years, several researchers in Software Engineering (SE) and
Programming Languages have been exploring the NLP techniques and
models to build automated SE tools. SMT has been directly used or
adapted to be used to translate/migrate source code in different
programming
languages~\cite{fse13-nier,icse14-demo,karaivanov14,ase15,icsme16}. The
problem is called {\em code migration}. In the modern world of
computing, code migration is important. Software vendors often develop
a product for multiple operating platforms in different languages. For
example, a mobile app could be developed for iOS (in Objective-C),
Android (in Java), and Windows Phone (in C\texttt{\#}).
%Rather than developing each version of a software product
%independently, it would be more economical to develop the product in
%one platform/language and then migrate to another.
Thus, there is an increasing need for migration of source code from
one programming language to another.
%

Unlike natural-language texts, source code follows syntactic rules and
has well-defined semantics with respect to programming languages. A
natural question is {\em how effective BLEU score is in evaluating the
  results of migrated source code in language migration}. The answer
to this question is important because if it does, we could~establish
an automated metric to evaluate the quality of SMT-based code
migration tools. Unfortunately, there has not yet any empirical
evidence to either validate or invalidate the effectiveness of BLEU
score in applying to source code in language migration.

Because BLEU score measures the lexical phrase-to-phrase
translation while source code has well syntax and dependencies, we
hypothesized that {\em BLEU score is not effective in evaluating
  migrated source code}.
%
We proved this hypothesis by contradiction. First, we assumed that
{\em BLEU score is effective in evaluating migrated source code}.
%
With respect to the use of BLEU for the translation results by a
single model or its use to compare the results across models, the
above assumption leads to {\em two necessary conditions}
(1) BLEU reflects well the semantic accuracy of migrated source code
with regard to the original code when they are translated by a model,
and (2) the quality comparison between translation models can be drawn
by using the BLEU metric.
%
%
%With respect to the use of BLEU for the translation results by a
%single model or its use to compare the results across models, this key
%hypothesis can further be divided into two parts: (1) BLEU does not
%reflect well the semantic accuracy of migrated source code with regard
%to the original code when they are translated by a model, and (2) the
%quality comparison between translation models cannot be drawn by using
%the BLEU metric.
%
We then conducted experiments to provide counterexamples to
empirically invalidate those two conditions. We choose the code
migration task because it is a popular SE task that applies SMT.

For the first condition, we chose the migration models that focus on
phrase translation, for example, lpSMT~\cite{fse13-nier} that adapts a
phrase-to-phrase translation model~\cite{phrasal10}. This type of
models produces migrated code with high lexical accuracy, \ie high
correctness for sequences of code tokens. However, several tokens or
sequences of tokens are placed in incorrect locations.  This results
in a higher BLEU score but with a lower semantic accuracy in migrated
code. We also chose the migration models that focus on
structures. Specifically, we picked mppSMT tool~\cite{ase15} that has
high semantic accuracy but with a wide range of BLEU
scores. Importantly, we aimed to show that BLEU has weak correlation
with semantic accuracy of the translated code.
%
For the second condition, we constructed an artificial model, p-mppSMT, from
mppSMT~\cite{ase15} that applies the idea introduced by Callison-Burch
{\em et al.}~\cite{Callison}. The important property of p-mppSMT is
that given a method, the translated result achieves the same BLEU
score as the result migrated by mppSMT. However, a minor lexical
difference in these translated methods might result in considerable
semantic differences. This could lead to translated results with the
same BLEU score, but they are functionally different.
%
Finally, because the two necessary conditions are false,
the assumption statement is false, thus, BLEU score is ineffective in
evaluating migrated source code.

%
%For second part, we chose the two SMT-based models belonging to the two
%popular groups of SMT: mppSMT~\cite{ase15}, which is based on phrase-based
%SMT, and GNMT~\cite{gnmt}, which is based on neural network models.
%We showed that an improvement in BLEU is not sufficient nor necessary
%to reflect an improvement in the quality of code migration. Therefore,
%BLEU cannot be used to compare SMT-based code migration models.
%

%  reflect well the semantic accuracy on the migrated source code}
%syntactic and semantics, we hypothesize that {\em BLEU metric does not
%Because the BLEU metric
%measures the phrase-to-phrase translation while source code has well
%syntactic and semantics, we hypothesize that {\em BLEU metric does not
%  reflect well the semantic accuracy on the migrated source code}. We
%conducted experiments to provide counterexamples to empirically
%validate that hypothesis. We choose the language migration task
%%from Java to C\texttt{\#}
%because it is a popular SE task that applies SMT. We show that under
%some circumstances an improvement in BLEU is not sufficient to reflect
%an improvement in code migration quality, and in other circumstances
%that it is not necessary to improve BLEU in order to achieve an
%improvement in migration quality.
%
%Specifically, for the sufficiency, we chose the migration models that
%focus on phrase translation, for example, fpSMT~\cite{fse13-nier} that
%adapts a phrase-to-phrase translation model~\cite{phrasal10}. This
%type of models produces migrated code with high lexical accuracy,
%i.e., high correctness for sequences of code tokens. However, several
%tokens or sequences of tokens are placed in the incorrect locations.
%This results in an improvement in BLEU but with a lower semantic
%acccuracy in migrated code. For the necessary part, we chose the
%migration models/tools that focus on structures. Specifically, we
%picked mppSMT tool~\cite{ase15} that has high semantic accuracy but
%with a wide range of BLEU~scores.

In our experiment, we used a dataset of 34,209 pairs of methods in 9
projects that were manually migrated from Java to C\texttt{\#} by
developers. The dataset was used in evaluating existing code migration
models~\cite{ase15}. We ran those above SMT-based migration models on
those methods. We then manually investigated 375 randomly-selected
pairs of methods for each model. For each pair of the manually
migrated method and the automatically migrated one from a tool, we
assigned a semantic accuracy score and computed the BLEU score. The
semantic accuracy score was given by a developer after examining the
original code in Java, the manually migrated code in C\texttt{\#} and
the auto-migrated code in C\texttt{\#} by a tool. In total, 2,250
translated results were manually checked and assigned with semantic
scores in about 45 hours. For the necessary condition (1), we then
computed the correlation between the BLEU scores and semantic scores
of all the pairs.  Our result shows that the BLEU metric has a weak
correlation with the semantic accuracy of the migrated code. For the
necessary condition (2), we compared the translated results of the
same set of 375 methods for mppSMT~\cite{ase15} and its artificial
variant, p-mppSMT.
%mppSMT~\cite{ase15} and GNMT~\cite{gnmt}.
%
We showed that the equivalence of BLEU scores of models does not indicate
the equivalence in the quality of their translation results.
%In nearly half of those methods, an improvement of BLEU score does not
%indicate an improvement in semantic score, and vice-versa.
%Conclusion
%By the results of the two experiments,
From those results, we concluded that BLEU is not an effective~metric
in evaluating 
%translation quality of
SMT-based migration tools.

%Tien
%We propose a metric: GVED that measure the similarity between graph
%representation. The intuition.

%In this work, we also introduce, {\model}, a novel metric to evaluate
%the results of the SMT-based code migration tools. {\model} focuses on
%measuring the accuracy of the semantics of the code with respect to
%the reference code in the ground truth. That is, it measures how close
%the resulting code to the ground truth when the semantics of the code
%is considered. {\model} measures semantics via program dependence
%graph (PDG) as data and control dependencies among program entities
%are considered as the key elements in a program. We also integrate
%three different scores from lexical, syntactical, and semantic levels
%into a final {\model} score. The lexical and syntactical scores are
%measured via string edit distance and tree edit distance,
%respectively, between the resulting code and the reference one. The
%intention of the lexical and syntactic scores is for the resulting
%code that might {\em not} be lexically or syntactically correct. We
%also conducted experiments to evaluate {\model} as in the previous
%experiments for BLEU. Our result shows that the new metric {\model} is
%highly correlated to the human judgments on the semantic accuracy of
%the resulting migrated code. {\model} can also be used to compare
%different SMT-based code migration models as it can successfully
%measure 95\% of the cases of the change in translation quality.
%The contributions of this paper include:

In this work, we also introduce, {\model}, a novel metric to evaluate
the results of the SMT-based code migration tools. The intuition is
that the metric that measures the results in the higher abstraction
level, the better metric for reflecting the semantics
accuracy. However, in code migration, the translated code might miss
information that is required to construct the higher level
representations. Therefore, we propose a multi-level metric to
evaluate the quality of translated code, that integrates the
measurement in three representation levels of source code in a
multi-layer and increasing manner from lexeme (text), abstract syntax
tree (AST), to program dependence graph (PDG). For a translated
result, {\model} score is the similarity score between the source code
and the expected result in the highest representation level that can
be constructed for both versions. Furthermore,
%To evaluate the effectiveness of {\model},
%in assessing the quality of migrated source code compared to
%BLEU,
we also conducted two experiments to evaluate {\model} on the abilities to
reflect the semantic accuracy and the comparison in translation quality
between models. Our results show that the new metric {\model} is
highly correlated to the semantic accuracy of the resulting migrated
code. The average correlation coefficients between {\model} and
semantic accuracy is 0.775, in comparison to 0.583 of BLEU score.  Our
consistent results of {\model} with semantic scores also demonstrate
the effectiveness of {\model} in comparing SMT-based code migration
models.

%The contributions of this paper include:

In summary, in this paper, our main contributions are:

\begin{itemize}
	\item An empirical evidence to show that BLEU metric does not only reflect
well the semantic accuracy of the migrated code for SMT-based
migration tools, but is also not effective in comparing translation
quality of models.

	\item {\model}, a novel metric to evaluate the results of the SMT-based
code migration tools, that integrates the scores at the lexical,
syntactical, and semantic levels in source code.

	\item An extensive empirical study to evaluate {\model} on multiple SMT-based code migration systems.
\end{itemize}

%Our dataset is publicly available for evaluation.


%In this paper we give a number of counterexamples for Bleu��s
%correlation with human judgments. We show that under some
%circumstances an improvement in Bleu is not sufficient to reflect a
%genuine improvement in translation quality, and in other circumstances
%that it is not necessary to improve Bleu in order to achieve a
%noticeable improvement in translation quality.



%Machine Translation (MT) is the use of computer program to translate
%text or speech from one language to another. Bleu score evaluates the
%quality of MT by calculating the modified n-grams precision and also
%taking into account the length difference penalty. Traditionally, MT
%is only applied to natural language, but now it is also used for
%technical and programming language. One notable use of MT for SE tasks
%is Code Migration. Even with that adaptation, SE community still
%relies on Blue to evaluate the quality of MT. It is well known that
%there is a significant difference between natural language and
%programing language: programing language has structure, and
%well-defined syntax. This leads to a question as whether Blue score is
%suitable for SE task (Code Migration) or not. If it is, we could
%continue to use it. Otherwise, we need another metric that is more
%suitable for programing language. Hence, the answer to the question
%above will help researchers and developers build and evaluate MT-based
%Code Migration system better. Some has attempted to answer the
%question by stating informal arguments toward the use of Bleu for SE
%task \cite{}. However, up to date, there has not been any empirical
%evidences to formally address the problem.

%Bleu measures the lexical difference between machine generated code and referenced one. On the other hand, to measure the semantic similarity between them is the ultimate goal when evaluating quality of Code Migration system.


%Bleu was proved to be correlated with human judgments in natural language MT systems \cite {Papineni02}. However, Callison at el argued that we should not over-rely on Bleu score as an improvement in Bleu score is not sufficient nor necessary to show an in improvement in translation quality \cite {Callison06}. To validate the use of Bleu on SE tasks, we set up an experiment to manually judge the result of multiple MT systems and compare its to the Bleu score. Our result showed that Bleu score has weak correlation to human judgments across
%-----------------------


%\vspace{0.03in} {\em 1.}  \textbf{\code{BLEU}}. BLEU measures
%translation quality by the accuracy of translating $n$-grams to
%$n$-grams with various values of $n$ (phrases to phrases):

% \[\code{BLEU} = BP.{e^{\frac{1}{n}(\log {P_1} + ... + \log {P_n})}}~\cite{bleu}\]
%where $BP$ is the {\em brevity penalty value}, which equals to 1 if
%the total length (i.e. the number of words) of the resulting sentences
%is longer than that of the {\em reference sentences} (i.e. the correct
%sentences in the oracle). Otherwise, it equals to the ratio between
%those two lengths. $P_i$ is the metrics for the overlapping between
%the bag of $i$-grams (repeating items are allowed) appearing in the
%resulting sentences and that of $i$-grams appearing in the reference
%sentences. Specifically, if $S^{i}_{ref}$ and $S^{i}_{trans}$ are the
%bags of $i$-grams appearing in the reference code and in the
%translation code respectively, $P_i$ = |$S^{i}_{ref}$ $\cap$
%$S^{i}_{trans}$|/|$S^{i}_{trans}$|. The value of \code{BLEU} is
%between 0-1. The higher it is, the higher the translation quality.

%Since $P_i$ represents the accuracy in translating phrases
%with $i$ consecutive words, the higher the value of $i$ is used, the
%better \code{BLEU} measures translation quality. For example, assume
%that a translation \code{Tr} has a high $P_1$ value but a
%low~$P_2$. That is, \code{Tr} has high word-to-word accuracy but low
%accuracy in translating 2-grams to 2-grams (e.g. the word order might
%not be respected in the result). Thus, using both $P_1$ and $P_2$ will
%measure \code{Tr} better than using only $P_1$. If
%translation~sen\-tences are shorter, \code{BP} is smaller and
%\code{BLEU} is smaller. If they are too long and more incorrect words
%occur, $P_i$ values are smaller, thus, \code{BLEU} is smaller. $P_i$s
%are computed for $i$=1-4.
