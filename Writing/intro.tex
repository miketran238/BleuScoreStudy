\section{Introduction}
\label{sec:intro}

Statistical Machine Translation (SMT)~\cite{smtbook} is a
natural~language processing (NLP) approach that uses statistical
learning to derive the translation ``rules'' from a training data and
applies the trained model to translate a sequence from the source
language ($L_S$) to the target one ($L_T$). SMT produces translated
texts based on the statistical models whose parameters are trained
from a corpus of corresponding texts in two languages. SMT has
been successful in translating natural-language texts.  Google
Translate~\cite{googletranslate} is a SMT-based tool~that can accept
inputs in 15 natural languages and allows the translation of texts
into one of 53 languages. Microsoft Translator~\cite{mstranslator}
also supports instant translation for more than 40~languages.

The statistical machine translation community relies on the BLEU ({\em
  BiLingual Evaluation Understudy}) metric for the purpose of
evaluating SMT models and tools. {\em BLEU metric}, also called {\em
  BLEU~score}, measures translation quality by the accuracy of
translating text phrases to another with various phrases'
lengths. BLEU was shown to be highly correlated with human judgments
on the translated texts from natural-language SMT
tools~\cite{Papineni2002}. 
%
%However, there exists criticism on BLEU as Callison-Burch {\em et
%  al.}~\cite{Callison} argued that an improvement in BLEU metric is
%not sufficient nor necessary to show an improvement in translation
%quality. Despite such criticism, 
BLEU score remains one of the most popular automated and inexpensive
metrics to evaluate the quality of translation models.


%However, Callison at el argued that we should not over-rely on Bleu
%score as an improvement in Bleu score is not sufficient nor necessary
%to show an in improvement in translation quality \cite {Callison06}.

In recent years, several researchers in Software Engineering (SE) and
Programming Languages have been exploring the NLP techniques and
models to build automated SE tools. SMT has been directly used or
adapted to be used to translate/migrate source code in different
programming
languages~\cite{fse13-nier,icse14-demo,karaivanov14,ase15,icsme16}. The
problem is called {\em language migration}. In the modern world of
computing, language migration is important. Software vendors often
develop a software product for multiple operating platforms in
different languages. For example, the same mobile app could be
developed for iOS (in Objective-C), Android (in Java), and Windows
Phone (in C\texttt{\#}).
%Rather than developing each version of a software product
%independently, it would be more economical to develop the product in
%one platform/language and then migrate to another.
Thus, there is an increasing need for migration/translation
of source code from one programming language to another.
%

Unlike natural-language texts, source code follows syntactic rules and
has well-defined semantics with respect to programming languages. A
natural question is {\em how effective BLEU score is in~evaluating the
  results of migrated source code in language migration}. The answer
to this question is important because if it does, we could~establish
an automated metric to evaluate the quality of SMT-based code
migration tools. 
%Son: Em nghi khong nhat thiet phai hoi truc tiep cau nay
%, and otherwise, a relevant question should be raised:
%{\em what is an alternative metric?} 
Unfortulately, there has not yet
any empirical evidence to either validate or invalidate the
effectiveness of BLEU score in applying to source code in language
migration.

Because the BLEU metric measures the the phrase-to-phrase translation
while source code has well syntactic and dependencies  
%\textbf{Son: dependencies instead of semantics?}
%\textbf{Ngoc: sua thanh rigid semantics (programming language co tinh chat nay khac voi natural language 1 cau co the co nhieu nghia)}, 
we hypothesize that {\em BLEU metric is not effective in evaluating the translated
  results of migrated source code}. With respect to the use of BLEU
for the translation results by a single model or its use to compare
the results across models, this key hypothesis can further be divided
into two parts: (1) BLEU does not reflect well the semantic accuracy
of migrated source code with regard to the original code when they are
translated by a model, and (2) the improvement of a model over another
cannot be measured by the BLEU metric.
%
We conducted experiments to provide counterexamples to empirically
validate that hypothesis. We choose the language migration task
because it is a popular SE task that applies SMT. 

%\textbf{Son: What is the general principle of selections?}
%\textbf{Ngoc: popular migration models that used SMT}

For the first part, we chose the migration models that focus on phrase
translation, for example, lpSMT~\cite{fse13-nier} that adapts a
phrase-to-phrase translation model~\cite{phrasal10}. This type of
models produces migrated code with high lexical accuracy, i.e., high
correctness for sequences of code tokens. However, several tokens or
sequences of tokens are placed in the incorrect locations.  This
results in an increment in BLEU but with a lower semantic acccuracy in
migrated code. We also chose the migration models/tools that focus on
structures. Specifically, we picked mppSMT tool~\cite{ase15} that has
high semantic accuracy but with a wide range of BLEU
scores. Importantly, we aimed to show that BLEU has weak correlation
with semantic accuracy on translated source code. For second part, we chose 
the three most popular models: lpSMT~\cite{fse13-nier}, mppSMT~\cite{ase15}, 
and GNMT~\cite{tien}. We showed that an improvement in BLEU is not
sufficient nor necessary to reflect an improvement in the quality of
code migration.

%  reflect well the semantic accuracy on the migrated source code}
%syntactic and semantics, we hypothesize that {\em BLEU metric does not
%Because the BLEU metric 
%measures the phrase-to-phrase translation while source code has well
%syntactic and semantics, we hypothesize that {\em BLEU metric does not
%  reflect well the semantic accuracy on the migrated source code}. We
%conducted experiments to provide counterexamples to empirically
%validate that hypothesis. We choose the language migration task 
%%from Java to C\texttt{\#} 
%because it is a popular SE task that applies SMT. We show that under
%some circumstances an improvement in BLEU is not sufficient to reflect
%an improvement in code migration quality, and in other circumstances
%that it is not necessary to improve BLEU in order to achieve an
%improvement in migration quality.
%
%Specifically, for the sufficiency, we chose the migration models that
%focus on phrase translation, for example, fpSMT~\cite{fse13-nier} that
%adapts a phrase-to-phrase translation model~\cite{phrasal10}. This
%type of models produces migrated code with high lexical accuracy,
%i.e., high correctness for sequences of code tokens. However, several
%tokens or sequences of tokens are placed in the incorrect locations.
%This results in an improvement in BLEU but with a lower semantic
%acccuracy in migrated code. For the necessary part, we chose the
%migration models/tools that focus on structures. Specifically, we
%picked mppSMT tool~\cite{ase15} that has high semantic accuracy but
%with a wide range of BLEU~scores.

In our experiment, we used a dataset of 34,209 pairs of methods
in 9 projects that were manually migrated from Java to
C\texttt{\#} by developers. The dataset was used for evaluating the
code migration models/tools in existing research~\cite{ase15}. We used
those above SMT-based migration tools to perform code migration for
those methods. We then manually investigated {\bf 375}
randomly-selected pairs of methods for each model. For each pair of the manually
migrated method and the automatically migrated one from a tool, we
assigned a semantic accuracy score and computed the BLEU score. The
semantic accuracy score was given by a developer after examining the
original code in Java, the manually migrated code in C\texttt{\#} and
the auto-migrated code in C\texttt{\#} by a tool. For the first part
of our hypothesis, we then computed the correlation between the BLEU
scores and semantic scores of all the pairs. Our result shows that the
BLEU metric has a weak correlation to the semantic accuracy of the
migrated code. For the second part of our hypothesis, we compared the
translated results of the same set of 375 methods for two different
SMT-based migration models. In nearly half of those methods, an
improvement of BLEU score does not indicate an improvement in semantic
score, and vice-versa.
%Conclusion
By the results of the two experiments, we concluded that BLEU is not a 
effective metric for evaluating the translated results of migrated code. 
%Tien
%We propose a metric: GVED that measure the similarity between graph
%representation. The intuition.

%In this work, we also introduce, {\model}, a novel metric to evaluate
%the results of the SMT-based code migration tools. {\model} focuses on
%measuring the accuracy of the semantics of the code with respect to
%the reference code in the ground truth. That is, it measures how close
%the resulting code to the ground truth when the semantics of the code
%is considered. {\model} measures semantics via program dependence
%graph (PDG) as data and control dependencies among program entities
%are considered as the key elements in a program. We also integrate
%three different scores from lexical, syntactical, and semantic levels
%into a final {\model} score. The lexical and syntactical scores are
%measured via string edit distance and tree edit distance,
%respectively, between the resulting code and the reference one. The
%intention of the lexical and syntactic scores is for the resulting
%code that might {\em not} be lexically or syntactically correct. We
%also conducted experiments to evaluate {\model} as in the previous
%experiments for BLEU. Our result shows that the new metric {\model} is
%highly correlated to the human judgments on the semantic accuracy of
%the resulting migrated code. {\model} can also be used to compare
%different SMT-based code migration models as it can successfully
%measure 95\% of the cases of the change in translation quality. 
%The contributions of this paper include:

In this work, we introduce, {\model}, a novel metric to evaluate
the results of the SMT-based code migration tools. 
The intuition is that the metric that measures the results 
in the higher abstraction level, the better metric for reflecting 
the semantics accuracy. However, in code migration, the translated code 
might be missed information that is required to construct the higher 
level representations. Therefore, we propose a multiple levels metric 
to evaluate the quality of translated code, that increasingly integrates 
the measuring in three representation levels of source code: lexeme 
(text), abstract syntax tree, and program dependence graph. For a 
translated result, the {\model} score is the similarity rate between 
the source code and the expected result in the highest level representations 
that can be constructed for both versions. To evaluate the effectiveness 
of {\model} in assessing the quality of migrated source code compared to BLEU, 
we conducted two experiments to evaluate {\model} on the abilities to reflect 
the semantic accuracy and the improvement of translation models. Our result 
shows that the new metric {\model} is highly correlated to the semantic 
accuracy of the resulting migrated code. {\model} can also be used to compare 
different SMT-based code migration models as it can successfully
measure 95\% of the cases of the change in translation quality. 
The contributions of this paper include:

1. An empirical evidence to show that BLEU metric does not reflect
well the semantic accuracy of the migrated code for SMT-based
migration tools.

2. {\model}, a novel metric to evaluate the results of the SMT-based
code migration tools, that integrates the scores at the lexical,
syntactical, and semantic levels in source code.

Our dataset is publicly available for evaluation.


%In this paper we give a number of counterexamples for Bleu��s
%correlation with human judgments. We show that under some
%circumstances an improvement in Bleu is not sufficient to reflect a
%genuine improvement in translation quality, and in other circumstances
%that it is not necessary to improve Bleu in order to achieve a
%noticeable improvement in translation quality.



%Machine Translation (MT) is the use of computer program to translate
%text or speech from one language to another. Bleu score evaluates the
%quality of MT by calculating the modified n-grams precision and also
%taking into account the length difference penalty. Traditionally, MT
%is only applied to natural language, but now it is also used for
%technical and programming language. One notable use of MT for SE tasks
%is Code Migration. Even with that adaptation, SE community still
%relies on Blue to evaluate the quality of MT. It is well known that
%there is a significant difference between natural language and
%programing language: programing language has structure, and
%well-defined syntax. This leads to a question as whether Blue score is
%suitable for SE task (Code Migration) or not. If it is, we could
%continue to use it. Otherwise, we need another metric that is more
%suitable for programing language. Hence, the answer to the question
%above will help researchers and developers build and evaluate MT-based
%Code Migration system better. Some has attempted to answer the
%question by stating informal arguments toward the use of Bleu for SE
%task \cite{}. However, up to date, there has not been any empirical
%evidences to formally address the problem.

%Bleu measures the lexical difference between machine generated code and referenced one. On the other hand, to measure the semantic similarity between them is the ultimate goal when evaluating quality of Code Migration system. 
 

%Bleu was proved to be correlated with human judgments in natural language MT systems \cite {Papineni02}. However, Callison at el argued that we should not over-rely on Bleu score as an improvement in Bleu score is not sufficient nor necessary to show an in improvement in translation quality \cite {Callison06}. To validate the use of Bleu on SE tasks, we set up an experiment to manually judge the result of multiple MT systems and compare its to the Bleu score. Our result showed that Bleu score has weak correlation to human judgments across 
%-----------------------


%\vspace{0.03in} {\em 1.}  \textbf{\code{BLEU}}. BLEU measures
%translation quality by the accuracy of translating $n$-grams to
%$n$-grams with various values of $n$ (phrases to phrases):

% \[\code{BLEU} = BP.{e^{\frac{1}{n}(\log {P_1} + ... + \log {P_n})}}~\cite{bleu}\]
%where $BP$ is the {\em brevity penalty value}, which equals to 1 if
%the total length (i.e. the number of words) of the resulting sentences
%is longer than that of the {\em reference sentences} (i.e. the correct
%sentences in the oracle). Otherwise, it equals to the ratio between
%those two lengths. $P_i$ is the metrics for the overlapping between
%the bag of $i$-grams (repeating items are allowed) appearing in the
%resulting sentences and that of $i$-grams appearing in the reference
%sentences. Specifically, if $S^{i}_{ref}$ and $S^{i}_{trans}$ are the
%bags of $i$-grams appearing in the reference code and in the
%translation code respectively, $P_i$ = |$S^{i}_{ref}$ $\cap$
%$S^{i}_{trans}$|/|$S^{i}_{trans}$|. The value of \code{BLEU} is
%between 0-1. The higher it is, the higher the translation quality.

%Since $P_i$ represents the accuracy in translating phrases
%with $i$ consecutive words, the higher the value of $i$ is used, the
%better \code{BLEU} measures translation quality. For example, assume
%that a translation \code{Tr} has a high $P_1$ value but a
%low~$P_2$. That is, \code{Tr} has high word-to-word accuracy but low
%accuracy in translating 2-grams to 2-grams (e.g. the word order might
%not be respected in the result). Thus, using both $P_1$ and $P_2$ will
%measure \code{Tr} better than using only $P_1$. If
%translation~sen\-tences are shorter, \code{BP} is smaller and
%\code{BLEU} is smaller. If they are too long and more incorrect words
%occur, $P_i$ values are smaller, thus, \code{BLEU} is smaller. $P_i$s
%are computed for $i$=1-4.
