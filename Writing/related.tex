\section{Related Work}

There exist many studies aiming to measure the functionality
similarity of source code, which utilize the similarities of
structures and 
dependencies~\cite{clone-tse07,roy09,baker97,ccfinder,cpminer,deckard,deckard2,horwitz01,baxter98}.
%ducasse99
Specifically, several approaches in the {\em code clone detection}
literature have been proposed~\cite{clone-tse07,roy09}. Generally,
they can be classified based on their code representations. The
typical categories are {\em text-based}~\cite{ducasse-icsm99}, {\em
  token-based}~\cite{baker97,ccfinder,cpminer,mende08}, {\em
  tree-based}~\cite{baxter98,deckard}, and
%evan07
{\em graph-based}~\cite{deckard2,horwitz01,liu06}.
%Mende \emph{et al.}~\cite{mende08} proposed a token-based similarity
%approach for grow-and-prune model in evolving software.

The text-based~\cite{ducasse-icsm99} and token-based~\cite{ccfinder}
approaches are efficient, scalable, and independent of the programming
languages. However, they cannot detect the code clones with different
syntactic structures. AST-based approaches~\cite{baxter98} overcome
that limitation but is language-dependent and has higher computational
complexity. Other approaches~\cite{fase09,deckard} make use of vector
computation to represent tree-based structures to reduce such
complexity.  Deckard~\cite{deckard} introduced the use of vectors in
clone detection. In Exas~\cite{fase09}, the authors showed that vector
representation for tree-based fragments is more generalized. Deckard
counts only distinct AST node types in a subtree for a fragment, while
Exas captures structural features via paths and sibling sets.
%However, they could not detect semantic clones.
%
%Jia \emph{et al.}~\cite{kclone} proposed the combination of lexical
%and local dependence analysis. 
%Cordy \emph{et al.}~\cite{cordy-cascon04} used lexical comparison
%tools and language-specific extractors to locate potential clones.
NICAD~\cite{nicad08} detects near-miss clones using flexible
pretty-printing and code~normalization.
%Other research aims to use visualization, code analysis, data mining
%techniques to support code clone
%management~\cite{girba-icpc06,ahmed-scam07,inoue07,lanza-wcre04,jeffgray04}.
%Chilowicz \emph{et al.}~\cite{chi09} propose a signature for a subtree
%using a tree database fingerprint method.

Graph-based clone detection approaches, though providing clones of the
higher level of abstraction, are of high complexity in detecting
similar subgraphs. Krinke's tool detects code clones via a program
dependence graph (PDG)~\cite{krinke01}. It finds the maximal
isomorphic subgraphs in a PDG by calculating the maximal $k$-limited
path induced subgraphs. Such induced subgraphs are defined as the
maximal similar subgraphs that are induced by $k$-limited paths
starting at two vertices. Their approach is more heavy-weight than the
use of vector-based calculation after structural feature extraction in
Exas.


Our study
%on BLEU score on code migration
is similar in nature to Callison-Burch {\em et al.}~\cite{Callison}'s
in NLP area. There exists criticism on BLEU as the authors argued that
an improvement in BLEU metric is not sufficient nor necessary to show
an improvement in translation quality.

%Several approaches have been proposed to mine migration rules.
%MAM~\cite{zhong-icse10} mines API mappings via Transformation Graphs.
%AURA~\cite{aura-icse10} combines call dependency and text analysis to
%identify many-to-one and one-to-many change rules.
%HiMa~\cite{meng-icse12} matches each revision pair of a framework and
%aggregates those rules to obtain evolution
%rules. Twinning~\cite{nita-icse10} allows to specify changes to
%migrate code to use new~APIs.

The statistical NLP approaches have been applied in software
engineering. Hindle {\em et al.}~\cite{natural} use
$n$-gram~\cite{manning99} with lexical tokens to show that source code
has high repetitiveness. 
%
%Han {\em et al.} \cite{han-ase09} used Hidden Markov Model to infer
%the next token from user-provided abbreviations.  
Raychev {\em et al.}~\cite{ethz-pldi14} capture common sequences of
API calls with $n$-gram to predict next API call.
NATURALIZE~\cite{barr-codeconvention-fse14} is a $n$-gram-based
statistical model that learns coding conventions to suggest natural
identifier names and formatting conventions.  $n$-gram is also used to
find code templates for a task~\cite{jacob10}, for large-scale
code mining~\cite{sutton-msr13}, for model
testing~\cite{tonella-icse14}, etc.

Tu {\em et al.}~\cite{tu-fse14} improve $n$-gram model with caching
capability for recently seen tokens to increase next-token prediction
accuracy.
%
SLAMC~\cite{fse13} associates to code tokens the semantic
annotations including their token and data types.
%
Maddison and Tarlow~\cite{tarlow14} use probabilistic context free
grammars and neuro-probabilistic language models to build language
model for source code. Their model is generative and does not use
semantic context. Oda {\em et al.}~\cite{hide-ase15} uses SMT to
generate English and Japanese pseudo-code from source~code.
