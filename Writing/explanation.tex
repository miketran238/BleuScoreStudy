\subsection{Further insights}
\textbf{Experimental procedure}. To empirically investigate further the characteristics of {\model} in reflecting the semantic accuracy, we collect subsets of methods in the
set of translated results that {\model} reflects well and not well the 
quality of them. The subset of results, on that {\model} reflects well 
their translation quality, archive {\model} scores that correlate well to 
the semantic scores. In this work, we aim to collect in our sample set 
the largest subset of migrated results whose {\model} scores correlate 
highest to the semantic scores. The subset of results, on that {\model} 
reflects not well their translation quality, contains the remaining 
methods in the sample set.

%In this experiment, we aimed to empirically investigate further the
%characteristics of {\model} in reflecting the semantic accuracy in
%migrated results. To achieve that, we focused on the migrated code
%that {\model} works well, \ie {\model} scores reflect well the
%semantic accuracy with respect to the reference code. Therefore, we
%aimed to collect in our sample set the subset of migrated results whose 
%{\model} scores correlate highest to the semantic scores given by the human subject.

%Correlation coefficients are used in statistics to measure how strong
%a relationship is between two variable space. Concretely, in this case
%those variable space are semantic scores and {\model} scores.
%
%We aim to figure out specifics of a dataset in which {\model} can work
%well. Because {\model} was created to estimate the semantic of code
%translation, the dataset in which {\model} works well also is the
%dataset containing the high correlation between Semantic score and
%{\model}.

%\subsubsection{The Use of RANSAC}

To do that, we use RANSAC ~\cite{Fischler:1981:RSC:358669.358692} to select two subsets of sample set of results:  
1) the largest set of method that have highest correlation coefficient 
between {\model} scores and semantic scores of the elements (Subset 1); and 2) the remaining pairs in the sample set, they get inconsistent between semantic score and {\model} score (Subset 2).
%Technically, RANSAC estimates a global relation that fits the
%data, while simultaneously classifying the dataset into an inlier set
%including the data points consistent with the relation and an outlier
%set including points not consistent with the relation. More details
%can found in~\cite{Fischler:1981:RSC:358669.358692}.

%To do that, we used an algorithm called Random Sample Consensus
%(RANSAC)~\cite{Fischler:1981:RSC:358669.358692}. Given two spaces
%({\model} scores and semantic scores) for a dataset of 375 pairs of
%methods, RANSAC will return two sets: 1) a set of pairs called {\bf
%  GOOD\_SET} containing the largest number of pairs and having highest
%correlation coefficient between {\model} scores and semantic scores;
%and 2) the complementary set of {\bf GOOD\_SET}, called {\bf
%  BAD\_SET}, containing pairs with the scores inconsistent between two
%spaces. Technically, RANSAC estimates a global relation that fits the
%data, while simultaneously classifying the dataset into an inlier set
%including the data points consistent with the relation and an outlier
%set including points not consistent with the relation. More details
%can found in~\cite{Fischler:1981:RSC:358669.358692}.

%RANSAC is a simple, yet powerful, technique that is commonly applied
%to the task of estimating the parameters of a model, using data that
%may be contaminated by outliers. 
%
%RANSAC estimates a global relation that fits the data, while
%simultaneously classifying the data into a inlier set including points
%consistent with the relation and a outlier set including points not
%consistent with the relation.

%In our research scope, we use RANSAC as a mean of data classifier
%which returns a set of pairs called \textbf{GOOD\_SET} contributing to
%the correlation between {\model} and semantic score and another set of
%pairs called \textbf{BAD\_SET} inconsistent the correlation.

%Due to its ability to tolerate a large fraction of outliers, the algorithm is a popular choice for a variety of robust estimation problems.


%RANSAC assumes that the training data consists of inliers that can be explained with the model and outliers that are gross-erroneous samples which do not fit the model at all. So using outliers when training the model would increase our final prediction error, as they contain almost no information about the model. 



%\subsubsection{Experiments}
In order to \textbf{DO SOMETHING????}, we independently run RANCSAC a 
certain number times on the sample set of translated results. Table~\ref{table:RANSAC_experiments} shows the results of our 10 times, which contains the sizes of subsets, on that  {\model} reflect well semantic accuracy, and the corresponding correlation coefficients.
%
The size of each subset ranges from 224 to 315, while the their
correlation value is at high rate (between 0.93 and 0.95).  From the
table, we investigate the subset with the \textbf{median} of the correlation, 0.954874293, with the size of 308.

%Figure~\ref{fig:inliers_outliers} shows the result of experiment No5 gained from RANCSAC.

\begin{table}
	\caption{RANSAC experiments}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Experiment Number & Number of pairs & Correlations of inliers \\
		\hline
		10	& 315	& 0.934700727 \\		
		8	& 312	& 0.945424846 \\	
		9	& 312	& 0.947983773 \\
		4	& 310	& 0.948196013 \\
		{\cellcolor[gray]{.8}}5	& {\cellcolor[gray]{.8}}308	& {\cellcolor[gray]{.8}}0.954874293 \\
		7	& 244	& 0.955112418 \\	
		1	& 283	& 0.957661146 \\
		3	& 303	& 0.958122675 \\
		2	& 298	& 0.958659967 \\
		6	& 277	& 0.959811603 \\		
		\hline
	\end{tabular}
	\label{table:RANSAC_experiments}
\end{table}

%\begin{figure}[t]
%	\caption{An example: Dataset in experiment No5 classified into inlier points(green) and outlier point(yellow)}
%	\includegraphics[scale=0.4]{img/inliers_outliers.png}
%	\centering
%	\label{fig:inliers_outliers}
%\end{figure}
%
%Examining $\Theta$, we classify each migrated result into
%different groups with common characteristics. Our goal of
%classification is to find the characteristics of {\model} to see how
%it reflects correctly the semantic accuracy. For the $\Delta$,
%we also have the goal of understanding in which cases, {\model} does
%not work well.

%the pairs of these sets into different groups that have common
%characteristics.  The principle for classification is that we aim to
%understand in which phenomenon {\model} could work well and could not
%work properly from \textbf{GOOD\_SET} and \textbf{BAD\_SET},
%respectively.


\textbf{Subset 1}. On this set, the code features relating to dependency, 
syntax that contribute to the semantic of the translated methods are 
captured by {\model}. By manually analyzing Subset 1, we 
categorize them into the following main cases.

First, there are 32/308 cases that {\model} is able to capture program 
elements having the same functionality but the different lexical with 
the reference. This feature is only fully captured by using data dependency 
graph. For example, in fig.~\ref{fig:mppSMT_example}, the code translated
used loop \texttt{for} which is the same meaning with \texttt{foreach} 
in the  reference version. Furthermore, accessing to an array element could 
be implemented by different ways such as \texttt{part[k]} or using iterator.
%

Additionally, there are 66 translated methods that contain the main program 
elements such as APIs, method calls, variables reflected on the reference. 
even their usages are incorrect. By comparing tree representations - TRS, 
{\model} can capture these elements based on the common subtrees of translated 
and reference code. In function \texttt{TestParserGrammarThere} of Fig.~\ref{fig:samples}, 
variable \texttt{grammar}, \texttt{found} and method call \texttt{Assert.AreEqual} 
appear in both translated and reference version but value assignments are not 
incorrect.
%

Finally, in Subset 1, the translated result contains the same structure and data flow, but 
lexically differ from the reference. For example, in fig.~\ref{fig:samples}, the structure 
of \texttt{ContentAppend} in the translated version is similar to the reference one with 
condition branches \texttt{if} and throwing exception \texttt{IllegalPdfSyntaxException}. 
\textbf{I DONT UNDERSTAND THE FOLLOWING SENTENCES}.
{\model} can reflect semantic similarity by comparing code structure. By using GRS, 
{\model} can reflect similarity of source code in term of structure. Usually, when 
manually evaluate translation quality, developers have high regard for code structure. 
That is the reason for why {\model} can evaluate semantic well in this case. 
%There 
%is a majority of pairs falling into this group, \texttt{112} pairs in total of 
%\textbf{GOOD\_SET}, to be exact.


%\begin{enumerate}
%\item The translated code contains program elements with different lexical but same functionality with the reference. In this case, {\model} can capture these elements by using data dependency graph. For example, as can be seen in fig.~\ref{fig:mppSMT_example}, code translated by mppSMT used loop \textbf{for} which is the same meaning with \textbf{foreach} in the reference version. Besides, accessing to an array element could be implemented by  different ways such as \textbf{part[k]} or using iterator. The case number for this phenomenon is \textbf{32} over \textbf{308} pairs of the set.
%
%\item Translated code contains the main program elements such as API names, method calls, variables reflected on the reference. even their usages are incorrect. By comparing tree representations - TRS, {\model} can capture these elements based on the common subtrees of translated and reference code. In the function \textbf{TestParserGrammarThere} of Fig.~\ref{fig:samples}, variable \textbf{grammar}, \textbf{found} and method call \textbf{Assert.AreEqual} appear in both translated and reference version but value assignments are not incorrect. There are \textbf{66} pairs involved in this situation.
%
%\item Translated code contains the same structure and data flow with the reference. As can be seen from function \textbf{ContentAppend} Fig.~\ref{fig:samples}, the code structure in the translated version is similar to the reference with condition branches \textbf{if} and throwing exception \textbf{IllegalPdfSyntaxException}. {\model} can reflect semantic similarity by comparing code structure. By using GRS, {\model} can reflect similarity of source code in term of structure. Usually, when manually evaluate translation quality, developers have high regard for code structure. That is the reason for why {\model} can evaluate sematic well in this case. There is a majority of pairs falling into this group,  \textbf{112} pairs in total of \textbf{GOOD\_SET}, to be exact.
%\end{enumerate}



\begin{figure}[t]
	\centering
	%\begin{lstlisting}[basicstyle=\small\sffamily, stepnumber=1, numbers=left, language=Java, aboveskip=1pt,  belowskip=1pt, numbersep=-5pt]
	\lstinputlisting[basicstyle=\scriptsize\sffamily,language=Java]{samples.cs}
	%\end{lstlisting}
	\caption{Sample}
	\label{fig:samples}
\end{figure}

\textbf{Subset 2}. In general, {\model} tries to measure the similarity between the translated code 
and the references one in the highest applicable representation level of code. However, in our sample 
set, there are some cases that are incorrect syntax code but still meaningful in the human perspective. 
In these cases, the effectiveness level of {\model} is only achieved as the level of STS or BLEU. 

%The past Stephen Hawking said: "One of the basic rules of the universe is that nothing is perfect. Perfection simply does not exist". Although {\model} can detect and capture well in some cases listed above, {\model} still has some drawbacks. By analyzing method pairs from \textbf{BAD\_SET}, it can be seen that {\model} does not work well on incomplete code. It means that when it comes with the code having syntax errors, the RUBY's eveluation is not high accuracy. The reason is that when the code can not be compiled due to syntax errors, both PDGs and ASTs are not applicable and RUBY is built based on lexical. Measuring semantic using lexical has limits as mentioned in this work before like using BLEU. 


%\begin{table}[]
%	\centering
%	\caption{Inlier classification}
%	\label{table:inliers_result}
%	\begin{tabular}{|m{1cm}|m{3cm}|m{4cm}}
%		\hline
%		Type      & Category         & Description                                                                                                                    
%		\\
%		\hline
%		High behavior & IDENTIDFIED           & Code is the same with the reference                                                                                             \\
%		& SAME\_FUNC\_DIFF\_LEX & Code uses a variety expression for the same functionarity with the reference, e.g for vs foreach, array[i] vs array.get(i)\\
%		& SAME\_DATA\_FLOW      & Code has the same data flow with the reference but there are still incorrect pieces of code                                     \\
%		\hline
%		Low behavior  & SAME\_KEYWORDS        & Code has the same keywords with the reference, i.e API names, method calls, variables. However, their usage is incorrect           \\
%		& DISORDERED            & Code has some same pieces of code with the reference. However they are disordered                                               \\
%		& DIFFERENT             & Code is totally different from the reference \\
%		\hline                                                                                   
%	\end{tabular}
%\end{table}
%
%
%\begin{table}[]
%	\centering
%	\caption{Outlier classification}
%	\label{table:outliers_result}
%	\begin{tabular}{|m{1cm}|m{3cm}|m{4cm}}
%		\hline
%		Type      & Category         & Description                                                                                                                    
%		\\
%		\hline
%		High behavior &     INCOMPLETED       &  Code has a lot of syntax errors and it is not completed however it still has a majority of same piece with the reference which include unimportant information \\                                   
%		\hline
%		Low behavior  & ALGORITHM\_VARIETY        & Code has the same functionarity with the reference, however it has been implemented by other algorithm, the lexical and data flow is almost different           \\
%		\hline                                                                                   
%	\end{tabular}
%\end{table}
