\section{Introduction}
<<<<<<< HEAD
Machine Translation (MT) is the use of computer program to translate text or speech from one language to another. The most popular automatic metrics to evaluate quality of MT is Bleu score. Traditionally, MT is only applied to natural language, but now it is also used for technical and programming language. One notable usage of MT for SE tasks is code migration. Even with that adapation, SE community still relies on Blue score. This leads to a question as whether Blue score is suitable for SE tasks or not.

 
=======
Machine Translation (MT) is the use of computer program to translate text or speech from one language to another. Traditionally, MT is only applied to natural language, but now it is also used for technical and programming language. Even with that evolution, SE community still relies on Blue score as the most popular automated metrics to evaluate the quality of MT. This leads to a question as whether Blue score is suitable for SE tasks. 
>>>>>>> cf928fb43fa9a3e06fd78067a46707920ffc77e9
Bleu score evaluate the quality of MT by calculating the modified n-grams precision and also taking into account the length difference penalty. Bleu was proved to be correlated with human judgments in natural language MT systems \cite {Papineni02}. However, Callison at el argued that we should not over-rely on Bleu score as an improvement in Bleu score is not sufficient nor necessary to show an in improvement in translation quality \cite {Callison06}. To validate the use of Bleu on SE tasks, we set up an experiment to manually judge the result of multiple MT systems and compare its to the Bleu score. Our result showed that Bleu score has weak correlation to human judgments across 
\section{Background}
Background information and metrics formulation
\subsection{Bleu Score}
Bleu (bilingual evaluation understudy) uses the modified form of n-grams precision and length difference penalty to evaluate the quality of text generated by MT compared to referenced one.
\subsection{Lexical Score}
\subsection{Syntax Score}
\subsection{Semantic Score}

\section{Hypothesis and Examples}
Our hypothesis is that bleu score does not measure well the closeness in term of semantics between the reference and translated source code. 
\subsection{Bleu score is not sufficient}
\subsection{Bleu score is not necessary}
\section{Emperical Study}
We have two RQs:

RQ1: Does bleu score reflect semantic meaning of translated source code? \\
RQ2: If Bleu score does not reflect semantic meaning, to what extend of source code it represents?
\subsection{RQ1}
We study the correlation between bleu score and semantic score. Our results show that Bleu score and Semantic score have weak correlation.
\subsection{RQ2} 
If the answer to RQ1 is 'no', we would like to investigate to what extend of source code does bleu score represents.

\section{Approach and Evaluation}
Since Bleu score is not suitable for SE task (code migration), we propose a new metric RUBY to evaluate quality of machine translation. 
\section{Related Works}

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate



