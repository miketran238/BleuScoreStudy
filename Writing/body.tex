\section{Introduction}
Machine Translation (MT) is the use of computer program to translate text or speech from one language to another. Bleu score evaluates the quality of MT by calculating the modified n-grams precision and also taking into account the length difference penalty. Traditionally, MT is only applied to natural language, but now it is also used for technical and programming language. One notable use of MT for SE tasks is Code Migration. Even with that adaptation, SE community still relies on Blue to evaluate the quality of MT. It is well known that there is a significant difference between natural language and programing language: programing language has structure, and well-defined syntax. This leads to a question as whether Blue score is suitable for SE task (Code Migration) or not. If it  is, we could continue to use it. Otherwise, we need another metric that is more suitable for programing language. Hence, the answer to the question above will help researchers and developers build and evaluate MT-based Code Migration system better. Some has attempted to answer the question by stating informal arguments toward the use of Bleu for SE task \cite{}. However, up to date, there has not been any empirical evidences to formally address the problem. 

Bleu measures the lexical difference between machine generated code and referenced one. On the other hand, to measure the semantic similarity between them is the ultimate goal when evaluating quality of Code Migration system. 
 

Bleu was proved to be correlated with human judgments in natural language MT systems \cite {Papineni02}. However, Callison at el argued that we should not over-rely on Bleu score as an improvement in Bleu score is not sufficient nor necessary to show an in improvement in translation quality \cite {Callison06}. To validate the use of Bleu on SE tasks, we set up an experiment to manually judge the result of multiple MT systems and compare its to the Bleu score. Our result showed that Bleu score has weak correlation to human judgments across 

\section{Background}
\subsection{Machine Translation and Code Migration}
\subsection{Metrics}
Bleu (bilingual evaluation understudy) uses the modified form of n-grams precision and length difference penalty to evaluate the quality of text generated by MT compared to referenced one.

\section{Research Questions and Hypothesis}
\subsection{RQ1}
Does bleu score reflect semantic meaning of translated source code?
\subsection{RQ2} 
If the answer to RQ1 is 'no', is Bleu correlated to Lexical representation of code?
\subsection{RQ3} 
If the answer to RQ1 is 'no', is Bleu correlated to Syntaxtical representation of code?
\subsection{Our hypothesis}
Our hypothesis is that bleu score does not measure well the closeness in term of semantics between the reference and translated source code. 
\section{Methodology}
\subsection{Proof of Hypothesis}
\subsection{Data Collection}
\subsection{Settings and Metrics}
\section{Evaluation}
Since Bleu score is not suitable for SE task (code migration), we propose a new metric RUBY to evaluate quality of machine translation. 
\section{Proposal}
\section{Related Works}

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate



