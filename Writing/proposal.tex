\section{Proposed Metric}
Since BLEU did not reflect well the semantic similarity of migrated code and reference code, there is a need for an alternative metric that can fit better with the task. The nature of the task is to compare source code in term of programming language \rq semantics. To compare the two language bases, they must be represented in some ways. While a language is normally composed by three parts: vocabulary, grammar, and meaning; programming language which is a special kind of language also can be composed by the 3 parts: lexemes, syntax, and semantics. Therefore, we choose one representation for each of the above part to compare, in order to heuristically estimate the semantic similarity. Specifically, token, AST and PDG are representations of lexemes, syntax, and semantics respectively. The metrics to compare those representations are SED, TREED and GVED which were already defined in section 4. Those metrics are then evaluated to show how well they reflect Semantic score. 
\input{explore}
\subsection{Design}
In this subsection, we propose {\model}, a novel automated metric to evaluate SMT-based code migration tools that is suitable for source code, is language independence, can reflect well the semantic accuracy of translated results, and inexpensive to compute. {\model} is then validated on practical SMT-based code migration models to show its potential and effectiveness. 

\emph{Insight 1 :} In computer network, a best effort delivery describes a network service that does not guarantee the quality of delivery. Nevertheless, the service will make their "best effort" to try to deliver the packet with its available resources. Under some circumstances, the packet may be loss or delayed. Similarly, in the domain of SMT-based Code Migration problem, a translated method cannot be guaranteed to be compilable or be built into PDG. Regardless, we would try our best effort to measure its semantic similarity with respect to the reference method as accurate as possible. \\
\emph{Insight 2 :}
According to Table \ref{tab:summary} in section 5, there is a trend of increasing correlation coefficient with Semantic score when more complex levels of source code are used. Specifically, GVED has the highest correlation, and it follows by TREED, SED, BLEU respectively. Therefore, those metrics should have priority order when be used in evaluating translation results.  

From the two insights, we design {\model} as a best-effort multi-layered metric. {\model} is a combination metric that takes into consideration metrics at the lexical, syntactical, and semantic levels of source code while aims to measure the semantic accuracy of translated code in a reliable manner. If the translated code can be built into PDG, we calculate {\model} in term of graph vector edit distance (GVED). If the translated code cannot be built into PDG, but is compilable, {\model} is calculated as syntax tree edit distance (TREED). Lastly, if the translated code is not compilable, we use string edit distance (SED) to compute {\model}.

The formula of {\model} is presented in Algorithm \ref{ruby}. GVED, TREED, and SED were defined in section 4.3. $r$ is the reference code, $t$ is the translated code; in turn $r,t$ are tokenized into sequences to use in SED, are parsed into ASTs to use in TREED, and are built into PDGs to use in GVED.  
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\begin{algorithm}
\caption{Calculate {\model}}\label{ruby}
\begin{algorithmic}[1]
\If { $\mbox{GVED}\left(r,t\right) $ is applicable }
\State $\mbox{RUBY}\left(r,t\right) = \mbox{GVED}\left(r,t\right) $
\ElsIf { $\mbox{TREED}\left(r,t\right) $ is applicable }
\State $\mbox{RUBY}\left(r,t\right) = \mbox{TREED}\left(r,t\right) $
\Else 
\State $\mbox{RUBY}\left(r,t\right) = \mbox{SED}\left(r,t\right) $
\EndIf
\end{algorithmic}
\end{algorithm}

\input{proposalResult}

\begin{figure}
\caption{RUBY vs Semantic (lpSMT)}
\centering
\includegraphics{img/rubyvssem_lpSMT.png}
\label{fig:RubySemlpSMT}
\end{figure}

\begin{figure}
\caption{RUBY vs Semantic (mppSMT)}
\centering
\includegraphics{img/rubyvssem_mppSMT.png}
\label{fig:RubySemMppSMT}
\end{figure}

%BLEU has always been doing this and that....

%Researchers usually claim that an improvement in BLEU also meant an
%improvement in translation quality. So 

%BLEU has been used for not only evaluating the result but also tuning
%and developing SMT-based migration system.
%%From the results in section 5, BLEU did not reflect the semantic accuracy of source code. --> We need a better metrics to replace BLEU
%However, from the results in section 5, it can be concluded that BLEU
%did not reflect well the semantic accuracy of migrated source code
%since it has weak relation with human judgments. Therefore, we need a
%better metric in order to fit better with programming languages and
%code migration systems.
%%
%%Needed metric should be 
%%Reflect semantical meaning of sources code.
%%Automated
%%Low computation's cost
%%Independent of programming language
%%Independent of MT model's type
%Such a metric should have the following requirements:
%
%
%\emph{1}. The metric is more suitable for source code than BLEU. It
%can reflect the semantics of source code and the semantic accuracy of
%translation result. To fulfill that, the metric must have high
%correlation with human evaluation for the translation result.
%
%
%\emph{2}. The metric can be computed automatically and inexpensively in
%order to support the evaluation of migration results in the
%incremental development of SMT-based code migration systems. For
%example, the metric can be calculated quickly after each iteration of
%development so a system can be evaluated and tuned in a timely~manner.
%
%\emph{3}. The metric is independent of the programming languages and
%of the SMT models. A good and reliable metric must have consistent
%results for any languages and models so it can be applied universally
%to any SMT-based code migration systems.
%
%%What is Ruby and why Ruby is good.
%Considering all above requirements, we introduce {\model}, a novel automated metric that can reflect semantic accuracy of translated code. {\model} is also independent of programming languages and machine translation models used in migration system. {\model} measures the semantic accuracy of the resulting code with respect to reference code by comparing their Program Dependence Graph (PDG). PDG captures both the data and control dependencies among program entities. Because those dependencies play an important role in a program, we expect PDG can represent well the semantics of source code. Usually, comparing graph is expensive, but our approach makes it affordable. We estimate the graph edit distance by vectorizing the graph and calculating the vectors edit distance. Because of that, we can save the computational cost and make our approach scalable. Basically, every programming language code can be built into PDG. Hence, our metric can be used for Code Migration systems that migrate different programming languages. Lastly, the way {\model} is measured makes it independent of machine translation models. That means Code Migration systems that deployed different SMT models can still use our metric. Next, we go into details how to formalize the calculation of {\model}
%
%%To reduce the high computational cost, we vectorize the PDGs and calculate the vector difference to estimate the graph difference. This way, we would make sure that our model is practical and applicable in large scaled systems. 
%
%When applying MT on source code, there always exists the problem that the translated code is broken in term of syntax. Thus, it is impossible to build PDG or even compile those code. To cope with the problem, our model is designed as best-effort, layered metric  : If the translated code can be built into PDG, we calculate {\model} in term of graph edit distance. If the translated code cannot be built into PDG but is compilable, we calculate {\model} in term of syntax tree edit distance. If the translated code is not compilable, we calculate {\model} in term of string edit distance. We then represent about the 3 metrics graph/tree/string edit distances as follows:



