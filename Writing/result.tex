\section{Empirical Results on BLEU Scores}
\label{sec:bleuresult}

%In this section, we present the empirical results to validate our
%hypothesis that \textit{BLEU is not effective in evaluating
%  translation quality of source code migration task}.

\subsection{Correlation between BLEU scores and semantic scores}
To justify the first part of our hypothesis, BLEU score does not reflect well
the semantic accuracy of results translated by a particular model,
we show the relation between BLEU scores and human judgments via semantic scores.
We use Pearson's correlation coefficient~\cite{geek_2015} to gauge
how strong their relation is. The correlation coefficient has a value
between [-1, 1], where 1 indicates the strongest positive relation, -1
indicates the strongest negative relation, and 0 indicates no relation.

%TODO BLEU, n-gram, n=???
Fig.~\ref{fig:BleuSemlpSMT} and Fig.~\ref{fig:BleuSemMppSMT} show the
scatter plots between two metrics: BLEU and Semantic. Each point
represents the scores of a pair of methods where its $x$-axis value is
for BLEU scores and $y$-axis value is for semantic scores. The
correlation coefficient between BLEU and semantic scores for the model
mppSMT is 0.523 and for the model lpSMT is 0.570. These positive values
are closer to 0.5 than to 1.0. This means there is a {\em positive but weak
relation} between BLEU score and semantic score. The weak correlations %help me to check grammar
between the metrics on the results translated by lpSMT and mppSMT are
demonstrated in Fig.~\ref{fig:BleuSemlpSMT} and Fig.~\ref{fig:BleuSemMppSMT}.

%\emph{Observation 1:}

%TODO mark what we want to talk about on figures

\begin{figure}
\caption{BLEU scores vs Semantic scores (lpSMT model)}
\centering
\includegraphics[width=2.9in]{img/bleuvssemantic_lpSMT.png}
\label{fig:BleuSemlpSMT}
\end{figure}

\begin{figure}
\caption{BLEU scores vs Semantic scores (mppSMT model)}
\centering
\includegraphics[width=2.9in]{img/bleuvssemantic_mppSMT.png}
\label{fig:BleuSemMppSMT}
\end{figure}

\subsubsection{{\bf Higher BLEU does not necessarily reflect higher~semantic accuracy}}

In Fig.~\ref{fig:BleuSemlpSMT}, for many specific values of BLEU, the
corresponding semantic scores can spread out for a wide range. For
instance, the BLEU score of 0.75, the corresponding semantic scores
are from 0.25 to 1.
% \textbf{Ngoc: can u mark in the figure please}.
Thus, from this observation, we conclude that the results migrated by
these models with {\em high BLEU scores might not achieve high semantic
scores}.
%There are two reasons for this.
%

%In our sample set, these results can fall in two main cases.
There are two main reasons for this result in our dataset.  First, the
translated methods might have multiple correct phrases, but in an
incorrect order, those methods can be incorrect, even not compilable.
%useless and justified as so in human judgment.
%
For example, in Fig.~\ref{fig:issueexample2}, the translated method
misplaces the position of '\{', making the method have a low
semantic score, however, it has high BLEU score.
%
%Another reason for this implication is that resulting method does not capture the important
%program elements.
In other cases, the migrated results are incomplete code missing the
elements that are trivial for the translation model, yet important
with respect to the syntactic rules of the target language. For
example, the result contains mostly keywords and separators such as
\code{if}, \code{public}, \code{()}, but misses out the important
program elements such as function calls or variable names. In this
case, it will have low semantic score while having a moderate to high
BLEU score. These circumstances indicate the weakness of BLEU metric
in evaluating the translated results in programming language where
syntax rules are well-defined.



%\emph{Observation 2:} For a fixed value of Semantic score, there can
%be many associated BLEU values. Specifically, in the model lpSMT, with
%a Semantic Score of 1, the BLEU scores can vary greatly between 0-1,
%which is reflected on the top horizontal line of dots in the
%Figure~\ref{fig:BleuSemlpSMT}. Similarly, in
%Figure~\ref{fig:BleuSemMppSMT}, with the Semantic Score of 1, the BLEU
%scores are in the range of 0.5 to 1.

\subsubsection{{\bf Higher semantic accuracy is not necessarily reflected by
higher BLEU score}} In the results translated by mppSMT
(Fig.~\ref{fig:BleuSemMppSMT}), for a particular value of the semantic
score, there can be many corresponding BLEU values spreading out in
a~wide range. Specifically, with the semantic score of 1, the BLEU
scores can vary from 0.5 to 1.0. Thus, it can be concluded
that for this model, the migrated code achieving {\em higher semantic score does not necessarily have higher BLEU~score}.

%From this observation, it can be implied that translated code can have
%low BLEU score, but high Semantic score. This can be explained by two
%reasons.
From our data, we observe that there are two main reasons leading to
that result.
%
First, a translated method can use code structure different from the
reference one to perform the same
functionality. Fig.~\ref{fig:mppSMT_example} shows an example that got
a maximum semantic score, but has low BLEU score of 0.4. In this
example, the translated method uses a \code{for} loop instead of a
\code{foreach} loop as in the reference code. The second reason is the
whitespace issue. For example, in Fig.~\ref{fig:mppSMT_example}, the
translated code has the tokens \code{IsSimilar(}, but the reference
code has \code{IsSimilar (}. The former is interpreted as one token
while the latter is interpreted as two. This situation reduces
the precision on phrases, but the human subject still evaluated the
result with a high semantic score. By this experiment, we also
empirically verify that the focus on the lexical precision of BLEU
makes it unable to capture other code-related aspects such as program
dependencies that contribute to program semantics. This might lead
to the ineffectiveness of BLEU in reflecting the semantic accuracy of
translated results.

In brief, {\em BLEU does not reflect well the
semantics of source code, and it is not suitable for evaluating the 
semantic accuracy of the result from an SMT-based code migration model}.

%TODO split translated results
\begin{figure}[t]
\centering
%\begin{lstlisting}[basicstyle=\small\sffamily, stepnumber=1, numbers=left, language=Java, aboveskip=1pt,  belowskip=1pt, numbersep=-5pt]
\lstinputlisting[basicstyle=\scriptsize\sffamily,language=Java]{mppExample.cs}
%\end{lstlisting}
\caption{Translated results and the corresponding reference code}
\label{fig:mppSMT_example}
\end{figure}


\input{cross-models}



