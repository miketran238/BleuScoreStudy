\subsection{The use of BLEU in comparing models}
%TODO gonna be removed
In our experiment on results translated by mppSMT and p-mppSMT for 375 methods, 
the average BLEU score of these models are identical. Meanwhile, 
the overall semantic score of p-mppSMT is \textbf{0.62} that is lower than the 
average for mppSMT, \textbf{0.88}. That means the deviant of BLEU in comparing 
models is nearly \textbf{2} level of semantic score in our sample set. 

%TODO complete and describe t-test results
To evaluate the abilities of BLEU in comparing the 
translation quality of models, we show that the two models mppSMT and p-mppSMT have different semantic score despite having identical BLEU score. We perform the paired t-test on the sample with confident level of 0.95. Our null hypothesis is the semantic scores of two models are equal. The p-value of the t-test is 2.81E-85
which is much smaller than 0.05. Thus we reject our null hypothesis and conclude that the semantic scores of two models are different with confident level of 95\%.


In our dataset, there are two main reasons for these results. First,
in some results of mppSMT, these phrases are lexically different those
in the reference one, but they may carry correct syntactical or
semantic information, which makes these results achieve high semantic
score. However, for the results translated by p-mppSMT and having
similar BLEU scores, these meaningful phrases are swapped, thus, are
placed in incorrect locations. This leads to significantly decreasing
in semantic scores. For example, in Fig.~\ref{fig:mppSMT_example}, the
result translated by p-mppSMT which has the same BLEU score with the
result of mppSMT. Meanwhile, the result of mppSMT is semantically
correct, however, the result from p-mppSMT is not compilable.
%
Furthermore, although several result of mppSMT might contain phrases
that are lexically incorrect, these phrases have the information that
provides developers clues to fix the translated one. However, these
phrases in the corresponding results translated by p-mppSMT are moved
into new positions, which also makes the semantic score of these
results are much lower than that of mppSMT. For example,...
\textbf{NGOC, example for this reason}

In our work, we additionally conduct an other experiment in the same
manner on GNMT and lpSMT.  The experimental results also support our
hypothesis. That can be found on our website \cite{??}.

By empirical results, we conclude that \textit{BLEU is not effective
  in comparing the translation quality of SMT-based models}.



%\subsubsection{Theoretical Models}
%To prove the second part of our hypothesis, we conduct study to see if an improvement in BLEU score over models reflects the improvement in translation quality represented by human judgments of semantic score. For a same set of 375 original Java methods, the two models mppSMT and mppSMT2 generates two sets of 375 translated methods. Each set has its own BLEU scores and Semantic scores. 
%From the 375 pairs, we have 73 pairs that have identical BLEU scores but are different in term of lexical. 26 out of 73 (\textbf{35\%}) have lower semantic score. On average, semantic score of model mppSMT2 is lower than mppSMT's by 0.16. Given that our semantic scores have only 5 pivots range from 0 to 1, a deduction of 0.16 is a large margin as it nearly reduces the semantic score by one pivot point.  
%The model mppSMT2 would swap the positions of incorrect phrases in term of lexical. However, those phrases may still have semantic information as the model mppSMT is capable of produce code that are semantically correct but lexically incorrect. There are many cases that they are semantically incorrect but still help the whole translated code syntactically correct or provide users with good starting point for the migration task. We have showed that a theoretical model can achieve identical BLEU score but has lower translation quality compared to another. 
%\subsubsection{Practical Models}

%Using the same experiment set up, we have two sets of 375 translated methods from two models GNMT and mppSMT. From the sample, we choose pairs of results such that their GNMT's BLEU scores are higher than mppSMT's BLEU scores. There are 215 of such pairs. Among them, 79 pairs have lower semantic score. Then, we perform t-test with alpha confident level of 0.95 on that subset to see if their GNMT's semantic scores are also significantly higher or not. Our null hypothesis is: GNMT's semantic score are higher than mppSMT's one with confident level of 0.95. The results show a t-value of \textbf{-9.1} which is lower than the critical point of -1.97. It meant we would reject the null hypothesis that GNMT's semantic scores are higher than mppSMT's ones. Our result shows that an improvement in BLEU score does not lead to an improvement in translation quality. 
%To validate the use of BLEU in comparing different SMT-based code migration systems, we conduct study to see if an improvement in BLEU score over cross models reflects the improvement in translation quality represented by human judgments of semantic score between those models. For a same set of 375 original Java methods, the two models GNMT and mppSMT generates two sets of 375 translated methods. Each set has its own BLEU scores and Semantic scores.
%We prove that an improvement in BLEU score does not sufficient nor necessary lead to an improvement in semantic score by showing:
%
%1. From the sample, we choose pairs of results such that their GNMT's BLEU scores are higher than mppSMT's BLEU scores. Then, we perform t-test with alpha confident level of 0.95 on that subset to see if their GNMT's semantic scores are also higher or not. The results show a t-value of ... It meant we would reject the null hypothesis that GNMT's semantic scores are higher than mppSMT's ones. So, it can be concluded that an improvement in BLEU score is not sufficient to achieve a higher semantic score. 
%
%2.  From the sample, we choose pairs of results such that their mppSMT's semantic scores are higher than GNMT's semantic scores. Then, we perform t-test with alpha confident level of 0.95 on that subset to see if their mppSMT's BLEU scores are also higher or not. The results show a t-value of ... It meant we would reject the null hypothesis that mppSMT's BLEU scores are higher than GNMT's ones. So, it can be concluded that an improvement in BLEU score is not necessary to achieve a higher semantic score. 

%We ignored pairs of translated results if they have the same BLEU score or Semantic score (136 of the cases). For the remaining results, we found out that in \textbf{34\%} of the cases, the change in BLEU score contradicts the change in Semantic score. It means an improvement in BLEU score leads to a decrease in Semantic score and vice versa. In other words, if one function is translated by two migration models, one-third of the time, the result which has higher BLEU score actually has lower translation quality than the other.

%Because of the results above, BLEU is not reliable to use in comparing different SMT-based migration models.
